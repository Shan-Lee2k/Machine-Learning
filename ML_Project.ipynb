{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shan-Lee2k/Machine-Learning/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZPIR2lfpGTn"
      },
      "source": [
        "# 1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eboFusxxCKS",
        "outputId": "1354eab6-3a1c-4408-d70c-8142082068d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c33205361b0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset,ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "torch.__version__\n",
        "print(torch.cuda.is_available())\n",
        "# assert torch.cuda.get_device_name() == \"\"\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBjndVZzo_Tn"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BiiBKnsmp-j2"
      },
      "outputs": [],
      "source": [
        "# Helper function\n",
        "def show_image(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "pre_processing = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If cannot download CelebA dataset, get from GG Drive\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/Colab Notebooks/img_align_celeba.zip'\n",
        "extract_path = '/dataset/celeba/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Download CelebA dataset successfully!\")"
      ],
      "metadata": {
        "id": "gk0sPFhR2KET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CelebA\n",
        "celeba_dataset = datasets.ImageFolder(root='/dataset/celeba/', transform=pre_processing)\n",
        "celeba_dataset.targets = torch.tensor([1] * len(celeba_dataset))\n",
        "train_size = 50000\n",
        "test_size = 10000\n",
        "\n",
        "indices = torch.randperm(len(celeba_dataset)).tolist()\n",
        "train_indices = indices[:train_size]\n",
        "test_indices = indices[train_size:train_size + test_size]\n",
        "\n",
        "\n",
        "# Create custom dataset class to return the modified label\n",
        "class CelebAWithLabel(Dataset):\n",
        "    def __init__(self, dataset, label):\n",
        "        self.dataset = dataset\n",
        "        self.label = torch.tensor(label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, _ = self.dataset[index]  # Ignore the original label\n",
        "        return image, self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "# Create subsets with the custom dataset class\n",
        "celeba_train_subset = Subset(CelebAWithLabel(celeba_dataset, label=1), train_indices)\n",
        "celeba_test_subset = Subset(CelebAWithLabel(celeba_dataset, label=1), test_indices)\n",
        "\n",
        "# Verify the change\n",
        "print(type(celeba_train_subset[0][1]))  # Should print 1\n",
        "print(celeba_test_subset[0][1])   # Should print 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HAWlaRNGh_zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4Ru-GFpx4AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNSIH1NnqDOP"
      },
      "outputs": [],
      "source": [
        "CIFAR_10_train = datasets.CIFAR10(root='/content/dataset', train = True, download=True,transform = pre_processing)\n",
        "CIFAR_10_test = datasets.CIFAR10(root='/content/dataset', train = False, download=True, transform = pre_processing)\n",
        "# Label 0 (not Face) for CIFAR_10\n",
        "CIFAR_10_train.targets = torch.tensor([0] * len(CIFAR_10_train.targets))\n",
        "CIFAR_10_test.targets = torch.tensor([0] * len(CIFAR_10_test.targets))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(CIFAR_10_train[0][0].shape, CIFAR_10_train[0][1])\n",
        "print(celeba_train_subset[0][0].shape, celeba_train_subset[0][1])"
      ],
      "metadata": {
        "id": "3h0mkqXW0qnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFORMATION DATASET**"
      ],
      "metadata": {
        "id": "wArPDM1vj6Kt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3S-WKiMrBC5"
      },
      "outputs": [],
      "source": [
        "print(f\"CelebA dataset training: {len(celeba_train_subset)}\")\n",
        "print(f\"CelebA dataset testing: {len(celeba_test_subset)}\")\n",
        "print(f\"Type CelebA: {type(celeba_train_subset)}\")\n",
        "print(f\"CIFAR_100 dataset training: {len(CIFAR_10_train)}\")\n",
        "print(f\"CIFAR_100 dataset testing: {len(CIFAR_10_test)}\")\n",
        "print(f\"Type CIFAR_100: {type(CIFAR_10_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kết hợp 2 bộ dữ liệu\n",
        "train_dataset = ConcatDataset([celeba_train_subset,CIFAR_10_train])\n",
        "test_dataset = ConcatDataset([CIFAR_10_test, celeba_test_subset])\n",
        "# Tạo DataLoader cho bộ dữ liệu kết hợp\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "gaq3GLCEk8kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualize Data**"
      ],
      "metadata": {
        "id": "TPgSSbkEpjrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BASIC CNN**\n",
        "\n"
      ],
      "metadata": {
        "id": "hFu1vTtJuEfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Basic_CNN(nn.Module):\n",
        "\n",
        "    # Contructor\n",
        "    def __init__(self, out_1=12, out_2=24, out_3 = 48, out_4 = 72):\n",
        "        super(Basic_CNN, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels= out_1, kernel_size=3, stride = 2)\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=3, stride=2)\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels=out_2, out_channels=out_3, kernel_size=3, stride=2)\n",
        "        self.maxpool3=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.cnn4 = nn.Conv2d(in_channels=out_3, out_channels=out_4, kernel_size=3, stride=2)\n",
        "        self.maxpool4=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.fc1 = nn.Linear(out_4 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    # Prediction\n",
        "    def forward(self, x):\n",
        "        x = self.cnn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.cnn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.cnn3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.cnn4(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ynqyDYUyyjIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):\n",
        "\n",
        "    #global variable\n",
        "    N_test=len(test_dataset)\n",
        "    accuracy_list=[]\n",
        "    loss_list=[]\n",
        "    for epoch in range(n_epochs):\n",
        "        for x, y in train_loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            z = model(x)\n",
        "            loss = criterion(z, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_list.append(loss)\n",
        "\n",
        "        correct=0\n",
        "        #perform a prediction on the validation  data\n",
        "        for x_test, y_test in validation_loader:\n",
        "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "            model.eval()\n",
        "            z = model(x_test)\n",
        "            _, yhat = torch.max(z.data, 1)\n",
        "            correct += (yhat == y_test).sum().item()\n",
        "        accuracy = correct / N_test\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "    return accuracy_list, loss_list"
      ],
      "metadata": {
        "id": "k2bk610JgS0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using GPU to train\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = Basic_CNN(out_1=12, out_2=24, out_3 = 48, out_4 = 60)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size= 64)"
      ],
      "metadata": {
        "id": "tO02WKlNgooH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=5,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
      ],
      "metadata": {
        "id": "WDfqyB9YhOKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN with Batch Normalization**"
      ],
      "metadata": {
        "id": "VxPmVDTlg0Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_with_batch_norm(nn.Module):\n",
        "\n",
        "    # Contructor\n",
        "    def __init__(self, out_1=12, out_2=24, out_3 = 48, out_4 = 72):\n",
        "        super(CNN_with_batch_norm, self).__init__()\n",
        "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels= out_1, kernel_size=3, stride = 2)\n",
        "        self.conv1_bn = nn.BatchNorm2d(out_1)\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=3, stride=2)\n",
        "        self.conv2_bn = nn.BatchNorm2d(out_2)\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.cnn3 = nn.Conv2d(in_channels=out_2, out_channels=out_3, kernel_size=3, stride=2)\n",
        "        self.conv3_bn = nn.BatchNorm2d(out_3)\n",
        "        self.maxpool3=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.cnn4 = nn.Conv2d(in_channels=out_3, out_channels=out_4, kernel_size=3, stride=2)\n",
        "        self.conv4_bn = nn.BatchNorm2d(out_4)\n",
        "        self.maxpool4=nn.MaxPool2d(kernel_size=2, stride = 1)\n",
        "\n",
        "        self.fc1 = nn.Linear(out_4 * 5 * 5, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(2)\n",
        "\n",
        "\n",
        "    # Prediction\n",
        "    def forward(self, x):\n",
        "        x = self.cnn1(x)\n",
        "        x = self.conv1_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.cnn2(x)\n",
        "        x = self.conv2_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.cnn3(x)\n",
        "        x = self.conv3_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.cnn4(x)\n",
        "        x = self.conv4_bn(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NQaIdId2x8g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-batch\n",
        "model_batch=CNN_with_batch_norm(out_1=12, out_2=24, out_3 = 48, out_4 = 60)\n",
        "model_batch.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate)\n",
        "accuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=5,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
      ],
      "metadata": {
        "id": "BgMGl2S4y02w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "4GikM452g8Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list_normal = [loss.cpu().item() for loss in loss_list_normal]\n",
        "loss_list_batch  = [loss.cpu().item() for loss in loss_list_batch]"
      ],
      "metadata": {
        "id": "bxOB7H0ba4BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss CNN and CNN_batch\n",
        "plt.plot(loss_list_normal, 'b',label='loss normal cnn ')\n",
        "plt.plot(loss_list_batch,'r',label='loss batch cnn')\n",
        "plt.xlabel('iteration')\n",
        "plt.title(\"loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "pU9Me8lTan3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy CNN and CNN_batch\n",
        "plt.plot(accuracy_list_normal, 'b',label=' normal CNN')\n",
        "plt.plot(accuracy_list_batch,'r',label=' CNN with Batch Norm')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title(\"Accuracy \")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ync5DzRatA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RZPIR2lfpGTn"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}